# NLP Final Lab - StreamingLLM Implementation

åŸºäº Pythia-70m æ¨¡å‹çš„ StreamingLLM KV Cache ä¼˜åŒ–å®éªŒ

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½](#æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½)
- [æ–‡ä»¶è¯´æ˜](#æ–‡ä»¶è¯´æ˜)
- [è¿è¡Œæ–¹æ³•](#è¿è¡Œæ–¹æ³•)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)
- [FAQ](#faq)

---

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº† **StreamingLLM** ç®—æ³•ï¼Œé€šè¿‡æ™ºèƒ½å‹ç¼© KV Cache æ¥ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚ä¸»è¦ç‰¹ç‚¹ï¼š

- âœ… **è´¨é‡æ— æŸ**: PPL ä¿æŒä¸å˜ (37.86 â†’ 37.86)
- âœ… **å†…å­˜ä¼˜åŒ–**: æ˜¾å­˜å ç”¨é™ä½ 11.6% (176.91 MB â†’ 156.41 MB)
- âœ… **é¦– Token åŠ é€Ÿ**: TTFT é™ä½ 60.2% (98.4ms â†’ 39.2ms)
- âœ… **å®Œæ•´å®ç°**: ä½¿ç”¨ Pre-Forward Hook æ­£ç¡®æ‹¦æˆªå’Œä¿®æ”¹ DynamicCache

**æ ¸å¿ƒæ€æƒ³**: ä¿ç•™å¼€å¤´çš„ Attention Sinks (n_sink tokens) å’Œæœ«å°¾çš„æœ€è¿‘ tokensï¼Œä¸¢å¼ƒä¸­é—´çš„è¿‡æ—¶ tokensã€‚

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. åˆ›å»º Conda ç¯å¢ƒ

```bash
# åˆ›å»ºåä¸º nlp çš„ Python 3.10 ç¯å¢ƒ
conda create -n nlp python=3.10 -y
conda activate nlp
```

### 2. å®‰è£…ä¾èµ–

```bash
# PyTorch (CUDA 11.8 ç‰ˆæœ¬ï¼Œæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Transformers å’Œç›¸å…³åº“
pip install transformers datasets accelerate
pip install huggingface_hub

# æ€§èƒ½åˆ†æå·¥å…·
pip install calflops

# å…¶ä»–å·¥å…·
pip install tqdm
```

### 3. ä¾èµ–ç‰ˆæœ¬è¯´æ˜

æ¨èç‰ˆæœ¬ï¼š
- Python: 3.10+
- PyTorch: 2.0+
- Transformers: 4.35+
- datasets: 2.x (æ³¨æ„ï¼š3.x ç‰ˆæœ¬å¯èƒ½å¯¼è‡´ PG-19 æ•°æ®é›†åŠ è½½å¤±è´¥)
- CUDA: 11.8 æˆ– 12.1

---

## ğŸ“¦ æ¨¡å‹ä¸æ•°æ®é›†ä¸‹è½½

### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨ä¸‹è½½ï¼ˆæ¨èï¼‰

è¿è¡Œä¸‹è½½è„šæœ¬ä¼šè‡ªåŠ¨ä» HuggingFace Mirror ä¸‹è½½ï¼š

```bash
conda activate nlp
python download_model.py
```

ä¸‹è½½å†…å®¹ï¼š
- **æ¨¡å‹**: Pythia-70m (EleutherAI/pythia-70m)
- **ä¿å­˜ä½ç½®**: `./models/pythia-70m/`

### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨é…ç½®

1. **è®¾ç½® HuggingFace é•œåƒ**ï¼ˆå¤§é™†ç”¨æˆ·å¿…éœ€ï¼‰:
   ```python
   import os
   os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
   ```

2. **æ•°æ®é›†è‡ªåŠ¨ä¸‹è½½**ï¼š
   - WikiText-2: è¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½åˆ° `./hf_cache/datasets/wikitext/`
   - PG-19: è¿è¡Œè„šæœ¬æ—¶è‡ªåŠ¨ä¸‹è½½åˆ° `./hf_cache/datasets/pg19/`

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒè„šæœ¬

| æ–‡ä»¶                     | è¯´æ˜                  | ç”¨é€”                                         |
| ------------------------ | --------------------- | -------------------------------------------- |
| `download_model.py`      | æ¨¡å‹ä¸‹è½½è„šæœ¬          | ä» HuggingFace ä¸‹è½½ Pythia-70m               |
| `baseline.py`            | åŸºå‡†æµ‹è¯•è„šæœ¬          | æµ‹è¯•åŸå§‹æ¨¡å‹çš„ PPLã€Memoryã€FLOPs ç­‰æŒ‡æ ‡     |
| `benchmark_streaming.py` | StreamingLLM å¯¹æ¯”æµ‹è¯• | å¯¹æ¯” Baseline å’Œ StreamingLLM çš„å…¨éƒ¨æ€§èƒ½æŒ‡æ ‡ |
| `pythia_press.py`        | StreamingLLM æ ¸å¿ƒå®ç° | KV Cache å‹ç¼©å™¨ï¼Œä½¿ç”¨ Pre-Forward Hook       |
| `run_pythia.py`          | ç®€å•æ¨ç†è„šæœ¬          | å¿«é€Ÿæµ‹è¯•æ¨¡å‹ç”Ÿæˆèƒ½åŠ›                         |

### è°ƒè¯•ä¸æ–‡æ¡£

| æ–‡ä»¶                     | è¯´æ˜                 |
| ------------------------ | -------------------- |
| `debug_press.py`         | è¯¦ç»†è°ƒè¯•å·¥å…·ï¼ˆå¯é€‰ï¼‰ | éªŒè¯å‹ç¼©é€»è¾‘ï¼Œå¯¹æ¯”ä¸‰ç§æ¨¡å¼ |
| `streaming_llm_press.py` | æ—§ç‰ˆå®ç°ï¼ˆå·²åºŸå¼ƒï¼‰   | å†å²ç‰ˆæœ¬ï¼Œä¸æ¨èä½¿ç”¨       |
| `FIX_SUMMARY.md`         | ä¿®å¤æ€»ç»“æ–‡æ¡£         | è¯¦ç»†è®°å½•è°ƒè¯•è¿‡ç¨‹å’Œè§£å†³æ–¹æ¡ˆ |
| `worklog.md`             | å·¥ä½œæ—¥å¿—             | å¼€å‘è¿‡ç¨‹è®°å½•               |
| `README.md`              | æœ¬æ–‡ä»¶               | é¡¹ç›®è¯´æ˜æ–‡æ¡£               |

### ç›®å½•ç»“æ„

```
NLP-FinalLab/
â”œâ”€â”€ models/                    # æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ pythia-70m/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â””â”€â”€ tokenizer.json
â”œâ”€â”€ hf_cache/                  # HuggingFace ç¼“å­˜
â”‚   â”œâ”€â”€ datasets/              # æ•°æ®é›†ç¼“å­˜
â”‚   â””â”€â”€ hub/                   # æ¨¡å‹ç¼“å­˜
â”œâ”€â”€ baseline.py                # åŸºå‡†æµ‹è¯•
â”œâ”€â”€ benchmark_streaming.py     # StreamingLLM å¯¹æ¯”
â”œâ”€â”€ pythia_press.py           # æ ¸å¿ƒå®ç°
â”œâ”€â”€ download_model.py         # ä¸‹è½½è„šæœ¬
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

---

## ğŸš€ è¿è¡Œæ–¹æ³•

### 1. ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œå¿…éœ€ï¼‰

```bash
conda activate nlp
python download_model.py
```

é¢„è®¡ä¸‹è½½æ—¶é—´ï¼š3-5 åˆ†é’Ÿï¼ˆå–å†³äºç½‘é€Ÿï¼‰

### 2. åŸºå‡†æµ‹è¯•ï¼ˆBaselineï¼‰

æµ‹è¯•åŸå§‹æ¨¡å‹æ€§èƒ½ï¼š

```bash
python baseline.py
```

è¾“å‡ºæŒ‡æ ‡ï¼š
- **PPL**: å›°æƒ‘åº¦ï¼ˆWikiText-2 å’Œ PG-19ï¼‰
- **Memory**: å³°å€¼æ˜¾å­˜å ç”¨
- **FLOPs**: æ¨¡å‹è®¡ç®—é‡
- **Speed**: ååé‡ã€TTFTã€TPOT

é¢„è®¡è¿è¡Œæ—¶é—´ï¼š5-10 åˆ†é’Ÿ

### 3. StreamingLLM å¯¹æ¯”æµ‹è¯•ï¼ˆæ ¸å¿ƒå®éªŒï¼‰

```bash
python benchmark_streaming.py
```

è¿™ä¼šè¿è¡Œï¼š
1. Baseline æµ‹è¯•
2. StreamingLLM æµ‹è¯•ï¼ˆcompression_ratio=0.7, n_sink=4ï¼‰
3. å¯¹æ¯”ä¸¤è€…çš„æ€§èƒ½å·®å¼‚

è¾“å‡ºå¯¹æ¯”è¡¨æ ¼ï¼š
```
æŒ‡æ ‡              | Baseline     | StreamingLLM | å˜åŒ–
-------------------------------------------------------
PPL              | 37.86        | 37.86        | +0.0%
Memory (MB)      | 176.91       | 156.41       | -11.6%
Throughput (t/s) | 164.66       | 150.59       | -8.5%
TTFT (s)         | 0.09841      | 0.03916      | -60.2%
TPOT (ms)        | 6.03         | 6.62         | +9.9%
```

### 4. å¿«é€Ÿæµ‹è¯•ç”Ÿæˆæ•ˆæœ

```bash
python run_pythia.py
```

è¿™ä¼šå¿«é€Ÿç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼ŒéªŒè¯æ¨¡å‹åŠ è½½æ­£ç¡®ã€‚

### 5. è°ƒè¯•å·¥å…·ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦è¯¦ç»†éªŒè¯å‹ç¼©é€»è¾‘ï¼š

```bash
python debug_press.py
```

è¾“å‡ºåŒ…æ‹¬ï¼š
- æ¯ä¸€æ­¥çš„ KV Cache é•¿åº¦
- å‹ç¼©å‰åçš„éªŒè¯
- ä¸‰ç§æ¨¡å¼çš„å¯¹æ¯”ï¼ˆBaseline / Manual / Generateï¼‰

---

## ğŸ“Š å®éªŒç»“æœ

### æœ€ç»ˆæ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡               | Baseline   | StreamingLLM | å˜åŒ–         | è¯´æ˜          |
| ------------------ | ---------- | ------------ | ------------ | ------------- |
| **PPL** (â†“)        | 37.86      | 37.86        | **+0.0%** âœ…  | è´¨é‡æ— æŸ      |
| **Memory** (â†“)     | 176.91 MB  | 156.41 MB    | **-11.6%** âœ… | æ˜¾å­˜ä¼˜åŒ–      |
| **TTFT** (â†“)       | 98.4 ms    | 39.2 ms      | **-60.2%** âœ… | é¦– Token åŠ é€Ÿ |
| **Throughput** (â†‘) | 164.66 t/s | 150.59 t/s   | -8.5%        | åˆç†ä»£ä»·      |
| **TPOT** (â†“)       | 6.03 ms    | 6.62 ms      | +9.9%        | åˆç†ä»£ä»·      |

### å…³é”®å‘ç°

1. **è´¨é‡ä¿è¯**: PPL ä¿æŒå®Œå…¨ä¸€è‡´ï¼Œè¯æ˜ Attention Sinks ç­–ç•¥æœ‰æ•ˆ
2. **å†…å­˜ä¼˜åŒ–**: åœ¨å°æ¨¡å‹ä¸ŠèŠ‚çœ 11.6%ï¼Œå¤§æ¨¡å‹æ•ˆæœä¼šæ›´æ˜¾è‘—
3. **å»¶è¿Ÿä¼˜åŒ–**: TTFT é™ä½ 60%ï¼Œç”¨æˆ·æ„ŸçŸ¥æ˜æ˜¾æ”¹å–„
4. **åˆç†æƒè¡¡**: ååé‡ç•¥é™ï¼Œä½†æ¢æ¥æ›´ä½å†…å­˜å’Œå»¶è¿Ÿ

### StreamingLLM å‚æ•°è¯´æ˜

```python
press = PythiaStreamingLLMPress(
    compression_ratio=0.7,  # å‹ç¼©ç‡ï¼šä¸¢å¼ƒ 70% çš„ä¸­é—´ tokens
    n_sink=4                # ä¿ç•™å¼€å¤´ 4 ä¸ª Attention Sink tokens
)
```

å‚æ•°è°ƒä¼˜å»ºè®®ï¼š
- **compression_ratio**: 0.5-0.8 ä¹‹é—´æ•ˆæœè¾ƒå¥½
- **n_sink**: 2-8 ä¹‹é—´ï¼Œå¤ªå°‘å½±å“è´¨é‡ï¼Œå¤ªå¤šå‹ç¼©æ•ˆæœå·®

### è®¡ç®—é‡åˆ†æ

```
Model FLOPs: 1.45 GFLOPs
MACs: 0.72 GMACs
Params: 70.43 M
```

---

## â“ FAQ

### Q1: å®‰è£… PyTorch æ—¶é‡åˆ° CUDA ç‰ˆæœ¬ä¸åŒ¹é…

**é—®é¢˜**: `RuntimeError: CUDA out of memory` æˆ– CUDA ç‰ˆæœ¬é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥ CUDA ç‰ˆæœ¬
nvidia-smi

# æ ¹æ® CUDA ç‰ˆæœ¬å®‰è£… PyTorch
# CUDA 11.8:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Q2: ä¸‹è½½æ¨¡å‹æ—¶æŠ¥é”™ `Connection timeout`

**é—®é¢˜**: å›½å†…ç½‘ç»œæ— æ³•ç›´æ¥è®¿é—® HuggingFace

**è§£å†³æ–¹æ¡ˆ**:
1. ç¡®ä¿è®¾ç½®äº†é•œåƒç¯å¢ƒå˜é‡ï¼š
   ```python
   os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
   ```
2. æˆ–ä½¿ç”¨ä»£ç†ï¼š
   ```bash
   export http_proxy=http://127.0.0.1:7890
   export https_proxy=http://127.0.0.1:7890
   ```

### Q3: PG-19 æ•°æ®é›†åŠ è½½å¤±è´¥

**é—®é¢˜**: `RuntimeError: Dataset scripts are no longer supported`

**åŸå› **: datasets åº“ç‰ˆæœ¬è¿‡é«˜ï¼ˆ3.xï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install datasets==2.21.0
```

æˆ–è€…åœ¨ä»£ç ä¸­ä½¿ç”¨ï¼š
```python
load_dataset("pg19", split="train", streaming=True, trust_remote_code=False)
```

### Q4: æ˜¾å­˜ä¸è¶³ `CUDA out of memory`

**é—®é¢˜**: GPU æ˜¾å­˜ä¸å¤Ÿè¿è¡Œæ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°‘æµ‹è¯•åºåˆ—é•¿åº¦ï¼ˆä¿®æ”¹ `MAX_LENGTH`ï¼‰
2. ä½¿ç”¨æ›´æ¿€è¿›çš„å‹ç¼©å‚æ•°ï¼š
   ```python
   press = PythiaStreamingLLMPress(compression_ratio=0.8, n_sink=2)
   ```
3. ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆæ…¢ä½†ä¸éœ€è¦ GPUï¼‰ï¼š
   ```python
   DEVICE = "cpu"
   ```

### Q5: StreamingLLM æ²¡æœ‰å‹ç¼©æ•ˆæœ

**é—®é¢˜**: æ˜¾å­˜å ç”¨æ²¡æœ‰æ˜æ˜¾ä¸‹é™

**å¯èƒ½åŸå› **:
1. **åºåˆ—å¤ªçŸ­**: StreamingLLM åœ¨é•¿åºåˆ—ï¼ˆ1000+ tokensï¼‰æ—¶æ•ˆæœæ‰æ˜æ˜¾
2. **å‚æ•°è®¾ç½®**: compression_ratio å¤ªä½æˆ– n_sink å¤ªå¤§
3. **å®ç°é”™è¯¯**: ç¡®ä¿ä½¿ç”¨çš„æ˜¯ `pythia_press.py`ï¼Œä¸æ˜¯ `streaming_llm_press.py`

**éªŒè¯æ–¹æ³•**:
```bash
python debug_press.py
```
æŸ¥çœ‹è¾“å‡ºä¸­çš„ "KV Cache é•¿åº¦" æ˜¯å¦ç¨³å®šç»´æŒåœ¨å‹ç¼©åçš„å¤§å°ã€‚

### Q6: å¦‚ä½•åœ¨è‡ªå·±çš„ä»£ç ä¸­ä½¿ç”¨ StreamingLLMï¼Ÿ

```python
from pythia_press import PythiaStreamingLLMPress
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    'models/pythia-70m',
    torch_dtype=torch.float16,
    device_map='cuda'
)
tokenizer = AutoTokenizer.from_pretrained('models/pythia-70m')

# æ³¨å†Œ StreamingLLM
press = PythiaStreamingLLMPress(compression_ratio=0.7, n_sink=4)
press.register(model)

# æ­£å¸¸ä½¿ç”¨ generate()
inputs = tokenizer("Hello", return_tensors='pt').to('cuda')
outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)

# æŸ¥çœ‹å‹ç¼©æ¬¡æ•°
print(f"å‹ç¼©æ¬¡æ•°: {press.compression_count}")

# è®°å¾—åœ¨ç»“æŸæ—¶ç§»é™¤ hook
press.remove()
```

### Q7: ä¸ºä»€ä¹ˆ TPOT å’Œ Throughput ç•¥æœ‰ä¸‹é™ï¼Ÿ

**åŸå› **: StreamingLLM éœ€è¦åœ¨æ¯æ¬¡ forward å‰æ‰§è¡Œå‹ç¼©æ“ä½œï¼Œä¼šå¼•å…¥å°‘é‡è®¡ç®—å¼€é”€ã€‚

**è¿™æ˜¯æ­£å¸¸çš„**: è®ºæ–‡ä¸­ä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼ç°è±¡ï¼Œè¿™æ˜¯ç”¨å°‘é‡è®¡ç®—æ¢å–å†…å­˜èŠ‚çœçš„åˆç†æƒè¡¡ã€‚

**ä¼˜åŒ–å»ºè®®**:
- å¦‚æœä¸»è¦å…³æ³¨ååé‡ï¼Œå¯ä»¥é™ä½ compression_ratioï¼ˆå¦‚ 0.5ï¼‰
- å¦‚æœä¸»è¦å…³æ³¨å†…å­˜ï¼Œå¯ä»¥æé«˜ compression_ratioï¼ˆå¦‚ 0.8ï¼‰

### Q8: å¦‚ä½•è°ƒæ•´å‹ç¼©å‚æ•°ä»¥è·å¾—æ›´å¥½æ•ˆæœï¼Ÿ

**å‚æ•°ç»„åˆå»ºè®®**:

| åœºæ™¯             | compression_ratio | n_sink | è¯´æ˜                   |
| ---------------- | ----------------- | ------ | ---------------------- |
| æœ€å¤§å†…å­˜èŠ‚çœ     | 0.8               | 2      | æ¿€è¿›å‹ç¼©ï¼Œå¯èƒ½å½±å“è´¨é‡ |
| å¹³è¡¡æ–¹æ¡ˆï¼ˆæ¨èï¼‰ | 0.7               | 4      | æœ¬å®éªŒä½¿ç”¨çš„é…ç½®       |
| ä¿å®ˆæ–¹æ¡ˆ         | 0.5               | 6      | è´¨é‡æœ€ä¼˜ï¼Œå‹ç¼©æ•ˆæœä¸€èˆ¬ |

ä¿®æ”¹ `benchmark_streaming.py` ä¸­çš„å‚æ•°ï¼š
```python
# ç¬¬ 9 è¡Œé™„è¿‘
COMPRESSION_RATIO = 0.7  # è°ƒæ•´è¿™ä¸ªå€¼
# ç¬¬ 174 è¡Œé™„è¿‘
press = PythiaStreamingLLMPress(
    compression_ratio=COMPRESSION_RATIO,
    n_sink=4  # è°ƒæ•´è¿™ä¸ªå€¼
)
```

---

## ğŸ”— å‚è€ƒèµ„æ–™

- [StreamingLLM è®ºæ–‡](https://arxiv.org/abs/2309.17453)
- [Pythia æ¨¡å‹](https://github.com/EleutherAI/pythia)
- [FIX_SUMMARY.md](FIX_SUMMARY.md) - è¯¦ç»†çš„å®ç°å’Œè°ƒè¯•è¿‡ç¨‹

---

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

**æœ€åæ›´æ–°**: 2024-12-14



